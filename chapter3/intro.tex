Early text simplification works break down the problem into sub-tasks, to make it more tractable. These sub-tasks include complex word and sentence identification, lexical simplification, syntactic simplification, and sentence splitting \citep{saggion2017automatic}. In this section, we focus on lexical simplification, the task of replacing difficult words in a text with words that are easier to understand. Lexical simplification can involve identifying complex words in context \citep{shardlow2013cw}, context-aware lexical substitution \citep{biran2011putting}, and finally choosing the appropriate simple word. This last step is often done by ranking words based on their frequency in a large general corpus \citep{devlin1998use}.

To identify complex words, we train a model on data manually annotated for complexity. Unlike previous work, our classifier takes into account both lexical and context features. We extract candidate substitutes for the identified complex words from SimplePPDB \citep{pavlick2016simple}, a database of 4.5 million English simplification rules linking English complex words to simpler paraphrases. We select the substitutes that best fit each context using a word embedding-based lexical substitution model \citep{melamud2015simple}. We show that our complex word identification and substitution model improves over several baselines which exploit other types of information and do not account for context. Our approach proposes accurate substitutes that are simpler than the target words and preserve the meaning of the corresponding sentences.



