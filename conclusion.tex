\documentclass[thesis.tex]{subfiles}
\begin{document}
\chapter{Conclusion}

The ability to automate the process of simplifying complex text is essential for children learning things for the first time, people with disabilities, second-language learners, and adults trying to shift to a new field in their career. Traditionally, the field of text simplification has addressed this as a generation task where given a complex text, a model needs to produce a simpler version that preserves its meaning. This is clearly a useful problem to solve, and also allows researchers to apply standard machine-translation-style generation models directly to this task. However, this framework does have inherent limitations; complexity is not a binary notion, and some phrases cannot just be replaced with simpler substitutes. This thesis has had three goals: to propose extensions to current simplification systems, while also identifying where these improvements still fall short; to systematically explore in detail the potential benefits of generating diverse candidates from a conditional language model; and, given the constraints of current simplification models, to propose a retrieval-based reformulation of the text simplification task to take steps towards a system that can be practically useful.

\section{Summary of Contributions}

The first part of this thesis focused on improving current lexical- and sentence-level text simplification systems. In Chapter 3, we focused on lexical simplification, which we broke down into two sub-tasks: identifying words that are difficult to understand in a text; and replacing them with simpler substitutes that make sense in context. For complex word identification, we train a Support Vector Machine classifier using word-based and context-based features, and evaluated its performance on our manually annotated dataset. We found that word-based features such as word length, frequency, and number of syllables already resulted in a competitive baseline, and while initial context-based features were not as useful, leveraging context via fine-tuning BERT resulted in further improvements. For the lexical simplification step, our proposed method revolved around extracting candidate substitutes from a large-scale simplification resource \citep{pavlick2016simple}, and ranking them based on an embedding-based substitution model that takes into account the surrounding context \citep{melamud2015simple}. We evaluated this approach on an evaluation set composed of aligned complex and simple sentence words, and found that it outperforms baselines that leverage other substitution resources or ranking mechanisms that do not rely on context

While in Chapter 3 we focus on a single text simplification sub-task, in Chapter 4 we attempt to holistically simplify entire sentences, leveraging a Sequence-to-Sequence (Seq2Seq) model to perform all simplification operations simultaneously. Previous work accurately noted that applying a generic Seq2Seq model to text simplification results in outputs that only slightly differed from the original sentence. Unlike previous Seq2Seq approaches, we propose ways to encourage the model to make operations that do not involve copying from the original during training, inference, and post-inference time, resulting in outputs that are shorter and simpler than in previous work. This does come at the cost of meaning preservation, which decreases human-annotated adequacy scores compared with the previous state-of-the-art of \cite{zhang2017sentence}, and we explore this in an additional experiment that varies our choice of output based on word length.

In the second section of Chapter 4, we explore a novel quality estimation metric to more accurately measure the perform of sentence simplification systems. We propose a quality estimation model that is based on fine-tuning a BERT-based model \citep{devlin2019bert,xenouleas2019sumqe} to simultaneously predict the fluency, adequacy, and relative complexity of an generated sentence simplification. We show that this results in a stronger correlation than previous metrics across all evaluation dimensions, and that training a single model to predict all three dimensions outperforms training three separate models.

In Chapter 4, our post-training extensions significantly improve the results, especially the mechanisms proposed for generating and re-ranking diverse candidate output sentences. In Chapter 5, we further explore the benefits of decoding diverse candidates from conditional language models, as this can have applications in different NLP generation tasks. We systematically compare a variety of diverse approaches that rely on beam-search and random sampling, on the tasks of conversational dialogue systems and image captioning. We find that outputs from diverse beam search methods are not actually that diverse, while outputs from random sampling methods contain significantly more diversity, but at the cost of quality.  We explore this quality/diversity trade-off further, and find that over-sampling and selecting candidates based on post-decoding clustering or on model perplexity results in a reasonable balance between quality and diversity.

In Chapters 3 and 4, we focused on established tasks in the simplification field. However, despite making improvements over previous approaches in both cases, closer analyses revealed that our methods still have a long way to go before we are able to generate a coherent and simpler version of an entire document. This is not a surprising conclusion, as even subsequent language models trained on huge amounts of text have issues with long-term dependencies \citep{ippolito2020automatic}. Thus, in Chapter 6 we propose to reformulate text simplification as a retrieval task. Given a document $D$, our goals in this setting are to identify the concepts that are critical for understanding $D$, to retrieve a set of documents related to each concept, and to re-rank these in order to find related documents written at a more appropriate complexity level for a user.

The first section of Chapter 6 focuses on critical concept identification. We create a custom evaluation corpus, using trained annotators who were asked to identify concepts within a set of Computer Science web articles. We then use this dataset to evaluate the performance of unsupervised statistical approaches and several BERT-based methods. Interestingly, we find that statistical baselines outperform unsupervised methods relying on pre-trained language models. The second section of Chapter 6 focuses on the task of retrieving aligned documents at different complexity levels. Our methodology leverages an initial re-ranking mechanism that predicts document complexity which filters out those that are not at the desired level, before ranking the documents based on their embedding-based similarity to the original document. We show that an embedding-based approach can give good results when the desired level is close to that of the original complex document, but our re-ranking step is critical in retrieving aligned articles written at significantly different complexity levels. We conduct experiments in settings of varying difficulty, involving a diverse number of distractor documents, and show that our methodology successfully retrieves related documents at the desired complexity level even in a challenging scenario with over one million candidate documents.

\section{Discussion and Future Work}

There are two key conclusions of this thesis. The first, from Chapters 4 and 5, is that diverse decoding is helpful in a variety of NLP generation tasks, but how this is implemented matters and depends on the underlying model quality. If the model used is generally low quality, then attempting to generate diverse candidates will likely result in outputs that are mostly unintelligible. On the other hand, if we assume that the model produces high-quality output, then we can take advantage of the decoding techniques that encourage more diversity, mainly constrained random sampling. In addition, we showed that oversampling prior to candidate selection allows a better balance between quality and diversity. 

The second conclusion is that while it is useful to continue improving upon the established task of sentence simplification, there may be a fundamental disconnect between this task and what is actually needed in order to help someone read a difficult article, or more generally, learn about an unfamiliar topic. Thus, in Chapter 6 this thesis proposes a retrieval-based reformulation of text simplification, showing how we can leverage both lighter weight and large-scale pre-trained models to identify concepts critical for understanding the content of a document, as well as for retrieving similar documents at different complexity levels. In order to continue to move the simplification field forward, it seems important to take a step back and think about the practical problems our models are actually trying to address.

This thesis leaves open several questions, which reflect the limitations of this work and open up opportunities for future research. First, more analysis should be done on the task-specific benefits and uses of generated diverse candidates. In the field of text simplification, there have been several recent works focusing on how to control the generated output, based on different priming tokens or sentence structures \citep{mallinson2019controllable,martin2020controllagle}. In a similar vein, generating diverse high-quality simplifications could allow us to then select which simplification is appropriate for a specific situation.

Another question is how to actually represent a document's overall complexity. Based on our experiments described in Section \ref{sec:comp_pred}, it is clear that we are able to accurately predict a document's complexity when training and testing on documents within a single corpus, but these models are relatively brittle when moving to another domain. To address this limitation, it would be interesting to consider more robust transfer learning options. Fine-tuning BERT is a type of transfer learning, but it still requires several hundred task-specific examples. In contrast, future work could consider leveraging models such as GPT-3 \citep{brown2020language}, which have shown to perform surprisingly well on few-shot and even zero-shot learning tasks, due to their vast increase in size over previous language models. This idea will naturally run into the problem of how to efficiently leverage models of this size in a practical setting, and also how to even train them in the first place. To begin to address this, future work can take inspiration from recent methods focusing on further optimizing the training and prediction of transformer-based models \citep{kitaev2020reformer}.

A natural extension of the retrieval experiments in Chapter 6 is to consider creating complexity-agnostic embedding representations, or representations that set aside specific dimensions for complexity, instead leveraging separate complexity and similarity ranking mechanisms. In this way, we would be able to fairly compare the similarity of any pair of texts, regardless of their complexity. One idea for doing this is to train a Generative Adversarial Network, or GAN; there has been recent work that creates two separate embedding representations of a text, one for the meaning of the text and one representing its style \citep{romanov2019adversarial}. One potential issue with this approach is that it is often very challenging to separate these two components. In Section \ref{sec:concept}, we discuss a similar idea, that complex words and phrases are often vital to understanding the overall meaning of a text. 

Finally, apart from gaining a better understanding of complexity in general, we also need to take into account the fact that different people approach the same text with different amounts of background knowledge. This significantly impacts the perceived complexity of that text. One way we can address this aspect is to have a personalized model of what people already know, based on what they have read in the past. However, even this is likely incomplete, because people learn about topics from many different media. This is clearly a difficult problem to solve, but a goal of the text simplification field should be to create models with a deeper understanding of this notion of user-dependent complexity. With this information, future work will be able to build powerful end-to-end resources to help anyone easily learn about a new topic, regardless of their background.

\biblio
\end{document}
