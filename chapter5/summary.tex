In this chapter, we perform an analysis of post-training decoding strategies that attempt to promote diversity in conditional language models. We show how over-sampling outputs and then filtering them down to the desired number is an easy way to increase diversity. Due to the computational expense of running large beam searches, we recommend using random-sampling to over-sample. The relative effectiveness of the various decoding strategies differs for the two tasks we considered, which suggests that the choice of optimal diverse decoding strategy is both task-specific and dependent on one's tolerance for lower quality outputs.

While we have focused on evaluating each decoding strategy under the specifics reported to be the best in the work that first proposed it, further work is necessary to conclude about whether the observed differences in quality and diversity may simply be due to the hyperparameters chosen in each paper. The ability to effectively generate a diverse set of responses while not degrading quality is extremely important in a variety of generation tasks, and is a crucial component to harnessing the power of state-of-the-art generative models.