In the previous chapter, we discussed how to more effectively apply Sequence-to-Sequence models to the task of sentence simplification, and found that encouraging diversity at inference time led to significant improvements. In this chapter, we pursue further the idea of diverse decoding, and perform an exhaustive comparison of current diverse decoding techniques.

As discussed in Section \ref{sec:seq2seq}, conditional neural language models output a probability distribution over the next token in the output sequence, given the input and the previously predicted tokens. Since computing the overall most likely output sequence is intractable, early work in neural machine translation found that beam search is an effective strategy to heuristically sample sufficiently likely sequences from these probabilistic models \citep{sutskever2014sequence}. However, for more open-ended tasks, beam search is ill-suited to generating a set of diverse candidate sequences; this is because candidates output by a large-scale beam search often only differ in punctuation and minor morphological variations \citep{li2016mutual}.

The term ``diversity" has been defined in a variety of ways in the literature, with some using it as a synonym for sentence ``interestingness" or ``unlikeliness" \citep{tatsunori2019unifying}, and others considering it a measure of how different two or more sentences are from each other \citep{vijayakumar2016diverse,gimpel2013systematic}. We take the latter approach, and define diversity as the ability of a generative method to create a set of possible outputs that are valid given the input, but vary as widely as possible in terms of word choice, topic, and meaning.

There are a number of reasons why it is desirable to produce a set of diverse candidate outputs for a given input. For example, in collaborative story generation, the system makes suggestions to a user for what they should write next \citep{clark2018creative}. In these settings, it would be beneficial to present the user with multiple different ways to continue their story. In image captioning, any one sentence-long caption is probably missing some information about the image. \cite{krause2017hierarchical} show how a set of diverse sentence-length image captions can be transformed into an entire paragraph about the image. Lastly, in applications that involve re-ranking candidate sequences, the re-ranking algorithms are more effective when the input sequences are diverse. Re-ranking diverse candidates has been shown to improve results in both open dialog \citep{li2016diversity,li2016mutual} and machine translation \citep{gimpel2013systematic}; we have also shown in Section \ref{sec:sentence} how we can leverage this for sentence simplification. Furthermore, in open-ended dialog, the use of re-ranking to personalize a model's responses for each user is a promising research direction \citep{choudhary2017domain}.

With these applications in mind, a variety of alternatives and extensions to beam search have been proposed which seek to produce a set of diverse candidate responses instead of a single high likelihood one \citep{li2016diversity,vijayakumar2016diverse,kulikov2018importance,tam2019clustered}. Many of these approaches show marked improvement in diversity over standard beam search across a variety of generative tasks. However, there has been little attempt to compare and evaluate these strategies against each other on a single task.

In this work, we review existing methods for promoting diversity in order to systematically investigate the relationship between diversity and perceived quality of sequences output by conditional language models. In addition to standard beam search and greedy random sampling, we compare several recently proposed modifications to both methods. In addition, we propose the use of over-sampling followed by post-decoding clustering to remove similar sequences, improving upon the methods from Section \ref{sec:sentence_diverse}.\footnote{This work was done in collaboration with Daphne Ippolito, Maria Kustikova, Jo\~ao Sedoc, and Chris Callison-Burch. Daphne and I share equal credit for this work, as we came up with the idea together and divided the work evenly.}