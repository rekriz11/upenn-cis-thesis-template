In the previous chapter, we focused on complex words, and their substitution with simpler words that preserve the original in-context meaning. In this chapter, we instead consider the task of simplifying an entire sentence, a task known as sentence simplification. Most recent sentence simplification research has approached this task as a monolingual machine translation problem, where the goal is to transform a complex English sentence into a simpler sentence that preserves the meaning of the original sentence \citep{zhu2010monolingual,narayan2014hybrid}. 

This chapter includes two sections. In the first section, we consider applying sequence-to-sequence (Seq2Seq) models to the task of sentence simplification, and propose various modifications to extend the generic framework during training, inference, and post-inference. We also provide an extensive error analysis to show where current sentence simplification models fall short. In the second section, we discuss how to fine-tune a BERT-based model on fluency, adequacy, and complexity simultaneously in order to predict the overall quality of sentence simplification system output without the need for references.